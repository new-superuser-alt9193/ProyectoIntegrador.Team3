{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Tecnológico de Monterrey**\n",
        "\n",
        "**Maestría en Inteligencia Artificial Aplicada**\n",
        "\n",
        "**ASIGNATURA: Proyecto Integrador**\n",
        "\n",
        "**PROFESOR TITULAR: Dra. Grettel Barceló Alonso**\n",
        "\n",
        "**ACTIVIDAD:Avance 4. Modelos Alternativos**\n",
        "\n",
        "---\n",
        "**EQUIPO 3**\n",
        "\n",
        "*   Rayan Bahrein García Fabián - A01204055\n",
        "*   Brandon Alexis del Ángel Gómez - A01795429\n",
        "*   Fernando Jiménez Pereyra - A01734609"
      ],
      "metadata": {
        "id": "1crwpxoE-Ukb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resumen ejecutivo\n",
        "\n",
        "### Objetivo\n",
        "\n",
        "Esta fase del proyecto tuvo como propósito evaluar diferentes algoritmos de aprendizaje supervisado aplicados al problema de predicción del contribution_score y su clasificación asociada, con el fin de identificar el modelo individual con mejor desempeño para su futura implementación.\n",
        "\n",
        "### 1. Construcción de modelos alternativos\n",
        "\n",
        "Se desarrollaron y evaluaron 14 modelos individuales, cubriendo tres enfoques:\n",
        "\n",
        "* Regresión (predicción continua de contribution_score):\n",
        "\n",
        "  * Algoritmos: LinearRegression, RandomForestRegressor, XGBRegressor, SVR, KNeighborsRegressor, DecisionTreeRegressor, GradientBoostingRegressor, LightGBMRegressor, MLPRegressor\n",
        "\n",
        "* Clasificación Multiclase (Bajo / Medio / Alto):\n",
        "\n",
        "  * Algoritmos: RandomForestClassifier, XGBoostClassifier\n",
        "\n",
        "* Clasificación Binaria (Alto vs No Alto):\n",
        "\n",
        "  * Algoritmos: LogisticRegression, RandomForestClassifier, XGBoostClassifier\n",
        "\n",
        "Todos los modelos fueron construidos con pipelines de preprocesamiento y validados con GridSearchCV, registrando tanto métricas de desempeño como tiempos de entrenamiento.\n",
        "\n",
        "\n",
        "### 2. Comparación de desempeño y selección\n",
        "\n",
        "* Mejor modelo: RandomForestRegressor\n",
        "\n",
        "  * MAE: 3.61 | R²: 0.80 | Tiempo: 2,576s\n",
        "\n",
        "  * Otros modelos destacados: SVR, LightGBMRegressor, XGBRegressor\n",
        "\n",
        "* Clasificación Multiclase:\n",
        "\n",
        "  * Mejor modelo: XGBoostClassifier\n",
        "\n",
        "  * Accuracy: 0.74 | F1 ponderado: 0.69 | Tiempo: 7s\n",
        "\n",
        "  * RandomForestClassifier mostró desempeño competitivo pero con mayor tiempo (138s)\n",
        "\n",
        "* Clasificación Binaria:\n",
        "\n",
        "  * Mejor modelo: LogisticRegression\n",
        "\n",
        "  * F1: 0.74 | AUC: 0.84 | Tiempo: 6.1s\n",
        "\n",
        "  * Le sigue RandomForestClassifier con AUC más alto (0.847), pero mayor tiempo de entrenamiento (80s)\n",
        "\n",
        "\n",
        "### 3. Justificación\n",
        "\n",
        "Se priorizó la selección de modelos que balancean desempeño predictivo, tiempo de entrenamiento y posible interpretabilidad, considerando también la facilidad de integración futura en entornos productivos.\n",
        "\n",
        "*    Para regresión, RandomForestRegressor fue elegido por su R² sobresaliente y estabilidad frente a no linealidades en los datos.\n",
        "\n",
        "*    Para clasificación multiclase, XGBoostClassifier demostró una ventaja en precisión y tiempo de entrenamiento.\n",
        "\n",
        "*    Para clasificación binaria, LogisticRegression ofrece una buena métrica F1, alto AUC, y es fácilmente interpretable.\n",
        "\n"
      ],
      "metadata": {
        "id": "GRBAG0CKI4z4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importación de librerías y carga de datos"
      ],
      "metadata": {
        "id": "N9skxbua-b4W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cd86e4d9-6709-4910-aa5d-2965e84fe080"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import joblib\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, RandomForestRegressor\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El dataset con el que trabajamos fue generado en los pasos anteriores de procesamiento, limpieza e ingeniería de características."
      ],
      "metadata": {
        "id": "UiPPYeWN-giq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cdda0ec-af20-45f5-9036-c45d30bdfebf"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('dataset_modelo_baseline.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generación de etiquetas objetivo y separación de conjuntos\n",
        "\n",
        "A partir de la variable continua `contribution_score` se generan dos etiquetas adicionales: una para clasificación multiclase (`score_label_multiclass`) y otra binaria (`bin_class`). Esto permite evaluar el rendimiento de distintos modelos según distintos enfoques: regresión, clasificación multiclase y binaria. Posteriormente, se separan las variables predictoras (`X`) de las variables objetivo para cada tipo de tarea supervisada."
      ],
      "metadata": {
        "id": "HFfzTLX0-zDQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c66ecf3c-9c26-464e-b220-da804ea1136b"
      },
      "outputs": [],
      "source": [
        "def clasificar_score(score):\n",
        "    if score < 65:\n",
        "        return 'Bajo'\n",
        "    elif score < 80:\n",
        "        return 'Medio'\n",
        "    else:\n",
        "        return 'Alto'\n",
        "\n",
        "df['score_label_multiclass'] = df['contribution_score'].apply(clasificar_score)\n",
        "# Crear variable binaria Alto vs No Alto (>=70)\n",
        "df['bin_class'] = (df['contribution_score'] >= 70).astype(int)\n",
        "# df['bin_class_label'] = df['bin_class'].map({1: 'Alto', 0: 'No Alto'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "430a9037-2c2c-4652-8f43-aae9937d14cb"
      },
      "outputs": [],
      "source": [
        "target_reg = 'contribution_score'\n",
        "X = df.drop(columns=[target_reg, 'score_label_multiclass', 'bin_class', 'bin_class_label',\n",
        "                     'project_id', 'employee_id', 'assignment_id'], errors='ignore')\n",
        "y_reg = df[target_reg]\n",
        "y_multi = df['score_label_multiclass']\n",
        "y_bin = df['bin_class']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## División de datos para entrenamiento y prueba\n",
        "\n",
        "Se realiza una división del conjunto de datos en entrenamiento y prueba, conservando la proporción de clases en la variable multiclase (`stratify=y_multi`). Esto es importante para evitar sesgos en el conjunto de validación y asegurar una evaluación representativa. El `random_state` garantiza reproducibilidad."
      ],
      "metadata": {
        "id": "bkY6LYCI_RuS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86700d09-74d2-4565-be74-8d3958e74fd8"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, yreg_train, yreg_test, ymulti_train, ymulti_test, ybin_train, ybin_test = train_test_split(\n",
        "    X, y_reg, y_multi, y_bin,\n",
        "    test_size=0.2, stratify=y_multi, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Procesamiento de variables numéricas y categóricas\n",
        "\n",
        "Se identifican las variables categóricas y numéricas para aplicar transformaciones específicas en un `ColumnTransformer`. Las variables numéricas se estandarizan con `StandardScaler`, y las categóricas se codifican con `OneHotEncoder`, ignorando categorías desconocidas. Esta transformación asegura que todas las variables estén en un formato compatible para el entrenamiento de modelos, sin introducir sesgos por escalas o categorías."
      ],
      "metadata": {
        "id": "G-zb6caP_cAD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d81d4579-f640-47e9-994b-cb2ae84f0c3f"
      },
      "outputs": [],
      "source": [
        "cat_cols = X.select_dtypes(include=['object']).columns\n",
        "num_cols = X.select_dtypes(include=['int64','float64']).columns\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', StandardScaler(), num_cols),\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)\n",
        "])\n",
        "\n",
        "# Ajustamos solo en X_train\n",
        "X_train_proc = preprocessor.fit_transform(X_train)\n",
        "X_test_proc  = preprocessor.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definición de modelos de regresión y búsqueda de hiperparámetros\n",
        "\n",
        "Se definen nueve modelos de regresión distintos utilizando algoritmos lineales, basados en árboles, redes neuronales, vecinos más cercanos y métodos de boosting. Cada modelo está acompañado de un conjunto de hiperparámetros para ser optimizados mediante `GridSearchCV`. Esto permite comparar tanto la precisión como la eficiencia de entrenamiento en una evaluación sistemática."
      ],
      "metadata": {
        "id": "xdRUAe4-_iYd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21bf190e-1151-4cea-8c83-89d469b88696"
      },
      "outputs": [],
      "source": [
        "model_configs_reg = {\n",
        "    'LinearRegression': (\n",
        "        LinearRegression(),\n",
        "        {}  # no hiperparámetros para grid\n",
        "    ),\n",
        "    'RandomForestRegressor': (\n",
        "        RandomForestRegressor(random_state=42),\n",
        "        {'reg__n_estimators': [100, 200]}\n",
        "    ),\n",
        "    'XGBRegressor': (\n",
        "        xgb.XGBRegressor(random_state=42),\n",
        "        {'reg__n_estimators': [100, 200], 'reg__max_depth': [3, 6]}\n",
        "    ),\n",
        "    'KNeighborsRegressor': (\n",
        "        KNeighborsRegressor(),\n",
        "        {'reg__n_neighbors': [3, 5, 7]}\n",
        "    ),\n",
        "    'SVR': (\n",
        "        SVR(),\n",
        "        {'reg__C': [0.1, 1, 10], 'reg__kernel': ['rbf', 'linear']}\n",
        "    ),\n",
        "    'DecisionTreeRegressor': (\n",
        "        DecisionTreeRegressor(random_state=42),\n",
        "        {'reg__max_depth': [None, 10, 20]}\n",
        "    ),\n",
        "    'GradientBoostingRegressor': (\n",
        "        GradientBoostingRegressor(random_state=42),\n",
        "        {'reg__n_estimators': [100, 200], 'reg__learning_rate': [0.05, 0.1]}\n",
        "    ),\n",
        "    'LightGBMRegressor': (\n",
        "        lgb.LGBMRegressor(random_state=42),\n",
        "        {'reg__n_estimators': [100, 200], 'reg__num_leaves': [31, 50]}\n",
        "    ),\n",
        "    'MLPRegressor': (\n",
        "        MLPRegressor(max_iter=500, random_state=42),\n",
        "        {'reg__hidden_layer_sizes': [(50,), (100,)], 'reg__alpha': [0.0001, 0.001]}\n",
        "    )\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento, ajuste y evaluación de cada modelo\n",
        "\n",
        "Para cada modelo, se construye un pipeline que incluye el preprocesamiento y el estimador. Se aplica `GridSearchCV` para identificar la mejor combinación de hiperparámetros usando validación cruzada (3-fold) con la métrica de error absoluto medio (`MAE`). Se registra el tiempo total de entrenamiento y se evalúa el modelo final sobre el conjunto de prueba utilizando `MAE` y `R²`, métricas estándar para tareas de regresión. Los resultados se almacenan en una lista para su posterior comparación."
      ],
      "metadata": {
        "id": "zt2T9FSK_olk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88d79755-5fe3-462b-8a0a-43821dd68cc4",
        "msg_id": "52aae80f-1897-4ead-a6d7-830efcee694c",
        "outputId": "003a8cc0-6f6c-429e-d9a3-ec000743e4f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entrenando regresor: LinearRegression\n",
            "LinearRegression: MAE=3.762, R2=0.709, Time=33.4s\n",
            "Entrenando regresor: RandomForestRegressor\n",
            "RandomForestRegressor: MAE=3.615, R2=0.801, Time=2577.0s\n",
            "Entrenando regresor: XGBRegressor\n",
            "XGBRegressor: MAE=4.391, R2=0.725, Time=5.3s\n",
            "Entrenando regresor: KNeighborsRegressor\n",
            "KNeighborsRegressor: MAE=5.815, R2=0.506, Time=40.6s\n",
            "Entrenando regresor: SVR\n",
            "SVR: MAE=3.937, R2=0.751, Time=3885.3s\n",
            "Entrenando regresor: DecisionTreeRegressor\n",
            "DecisionTreeRegressor: MAE=4.520, R2=0.656, Time=20.0s\n",
            "Entrenando regresor: GradientBoostingRegressor\n",
            "GradientBoostingRegressor: MAE=5.194, R2=0.635, Time=26.5s\n",
            "Entrenando regresor: LightGBMRegressor\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 39.780577 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2889\n",
            "[LightGBM] [Info] Number of data points in the train set: 16994, number of used features: 1138\n",
            "[LightGBM] [Info] Start training from score 68.718983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 43.285995 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2889\n",
            "[LightGBM] [Info] Number of data points in the train set: 16994, number of used features: 1138\n",
            "[LightGBM] [Info] Start training from score 68.718983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 43.527830 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2889\n",
            "[LightGBM] [Info] Number of data points in the train set: 16994, number of used features: 1138\n",
            "[LightGBM] [Info] Start training from score 68.718983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 47.350749 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2889\n",
            "[LightGBM] [Info] Number of data points in the train set: 16994, number of used features: 1138\n",
            "[LightGBM] [Info] Start training from score 68.718983\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 48.542841 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2871\n",
            "[LightGBM] [Info] Number of data points in the train set: 16994, number of used features: 1128\n",
            "[LightGBM] [Info] Start training from score 68.754719\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 54.053156 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2871\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 48.912829 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Number of data points in the train set: 16994, number of used features: 1128\n",
            "[LightGBM] [Info] Total Bins 2871\n",
            "[LightGBM] [Info] Number of data points in the train set: 16994, number of used features: 1128\n",
            "[LightGBM] [Info] Start training from score 68.754719\n",
            "[LightGBM] [Info] Start training from score 68.754719\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 53.777694 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2873\n",
            "[LightGBM] [Info] Number of data points in the train set: 16994, number of used features: 1128\n",
            "[LightGBM] [Info] Start training from score 68.730311\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 52.020532 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2873\n",
            "[LightGBM] [Info] Number of data points in the train set: 16994, number of used features: 1128\n",
            "[LightGBM] [Info] Start training from score 68.730311\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 58.643957 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2873\n",
            "[LightGBM] [Info] Number of data points in the train set: 16994, number of used features: 1128\n",
            "[LightGBM] [Info] Start training from score 68.730311\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 56.944043 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2871\n",
            "[LightGBM] [Info] Number of data points in the train set: 16994, number of used features: 1128\n",
            "[LightGBM] [Info] Start training from score 68.754719\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 59.566417 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2873\n",
            "[LightGBM] [Info] Number of data points in the train set: 16994, number of used features: 1128\n",
            "[LightGBM] [Info] Start training from score 68.730311\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/alt9193/Documents/act4/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n",
            "/home/alt9193/Documents/act4/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n",
            "/home/alt9193/Documents/act4/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n",
            "/home/alt9193/Documents/act4/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n",
            "/home/alt9193/Documents/act4/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n",
            "/home/alt9193/Documents/act4/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n",
            "/home/alt9193/Documents/act4/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n",
            "/home/alt9193/Documents/act4/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n",
            "/home/alt9193/Documents/act4/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n",
            "/home/alt9193/Documents/act4/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n",
            "/home/alt9193/Documents/act4/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n",
            "/home/alt9193/Documents/act4/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032966 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 4231\n",
            "[LightGBM] [Info] Number of data points in the train set: 25491, number of used features: 1803\n",
            "[LightGBM] [Info] Start training from score 68.734671\n",
            "LightGBMRegressor: MAE=4.257, R2=0.737, Time=1637.0s\n",
            "Entrenando regresor: MLPRegressor\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/alt9193/Documents/act4/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLPRegressor: MAE=4.812, R2=0.632, Time=1402.7s\n"
          ]
        }
      ],
      "source": [
        "results_reg = []\n",
        "best_models_reg = {}\n",
        "\n",
        "for name, (model, param_grid) in model_configs_reg.items():\n",
        "    print(f\"Entrenando regresor: {name}\")\n",
        "    pipe = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('reg', model)\n",
        "    ])\n",
        "    grid = GridSearchCV(pipe, param_grid, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
        "    start = time.time()\n",
        "    grid.fit(X_train, yreg_train)\n",
        "    elapsed = time.time() - start\n",
        "    best = grid.best_estimator_\n",
        "    y_pred = best.predict(X_test)\n",
        "    mae = mean_absolute_error(yreg_test, y_pred)\n",
        "    r2 = r2_score(yreg_test, y_pred)\n",
        "    results_reg.append({\n",
        "        'Model': name,\n",
        "        'BestParams': grid.best_params_,\n",
        "        'MAE': mae,\n",
        "        'R2': r2,\n",
        "        'TrainTime_s': elapsed\n",
        "    })\n",
        "    best_models_reg[name] = best\n",
        "    # joblib.dump(best, f'best_regressor_{name}.joblib')\n",
        "    print(f\"{name}: MAE={mae:.3f}, R2={r2:.3f}, Time={elapsed:.1f}s\")\n",
        "\n",
        "# Guardar resultados en CSV\n",
        "df_reg = pd.DataFrame(results_reg).sort_values('MAE')\n",
        "# df_reg.to_csv('regression_comparison_results.csv', index=False)\n",
        "# print(\"Resultados de regresión guardados en regression_comparison_results.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Consolidación de resultados\n",
        "\n",
        "Se consolidan los resultados de entrenamiento en un DataFrame ordenado por desempeño (MAE), lo cual permite identificar fácilmente los modelos más prometedores. Esta tabla es útil para documentar el rendimiento, hiperparámetros seleccionados y tiempo de entrenamiento, y servirá como base para seleccionar los dos mejores modelos para la etapa de ajuste fino."
      ],
      "metadata": {
        "id": "C1krQXvL_xYa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dddb3930-40bd-40cf-99eb-126abed18e25",
        "msg_id": "7f0e2958-f0eb-48c8-9cbf-32384724381e"
      },
      "outputs": [],
      "source": [
        "results_reg_df = pd.DataFrame(results_reg)\n",
        "results_reg_df.sort_values(by='R2', ascending=False, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5139a17-0eb1-4181-8f57-374dbe7a88d7",
        "msg_id": "bb64b0fb-ff01-49a1-a7a5-b5b764b80af7",
        "outputId": "55307c90-9396-4b21-ba60-edccff4534ef"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>BestParams</th>\n",
              "      <th>MAE</th>\n",
              "      <th>R2</th>\n",
              "      <th>TrainTime_s</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RandomForestRegressor</td>\n",
              "      <td>{'reg__n_estimators': 200}</td>\n",
              "      <td>3.614991</td>\n",
              "      <td>0.800781</td>\n",
              "      <td>2576.957614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVR</td>\n",
              "      <td>{'reg__C': 10, 'reg__kernel': 'linear'}</td>\n",
              "      <td>3.936745</td>\n",
              "      <td>0.751142</td>\n",
              "      <td>3885.292482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>LightGBMRegressor</td>\n",
              "      <td>{'reg__n_estimators': 200, 'reg__num_leaves': 50}</td>\n",
              "      <td>4.257365</td>\n",
              "      <td>0.736723</td>\n",
              "      <td>1637.035079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBRegressor</td>\n",
              "      <td>{'reg__max_depth': 6, 'reg__n_estimators': 200}</td>\n",
              "      <td>4.391310</td>\n",
              "      <td>0.724847</td>\n",
              "      <td>5.309225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LinearRegression</td>\n",
              "      <td>{}</td>\n",
              "      <td>3.762273</td>\n",
              "      <td>0.709092</td>\n",
              "      <td>33.442690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>DecisionTreeRegressor</td>\n",
              "      <td>{'reg__max_depth': None}</td>\n",
              "      <td>4.520351</td>\n",
              "      <td>0.655922</td>\n",
              "      <td>19.986360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GradientBoostingRegressor</td>\n",
              "      <td>{'reg__learning_rate': 0.1, 'reg__n_estimators...</td>\n",
              "      <td>5.193518</td>\n",
              "      <td>0.635306</td>\n",
              "      <td>26.459972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>MLPRegressor</td>\n",
              "      <td>{'reg__alpha': 0.001, 'reg__hidden_layer_sizes...</td>\n",
              "      <td>4.812238</td>\n",
              "      <td>0.632065</td>\n",
              "      <td>1402.680601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>KNeighborsRegressor</td>\n",
              "      <td>{'reg__n_neighbors': 5}</td>\n",
              "      <td>5.815198</td>\n",
              "      <td>0.506143</td>\n",
              "      <td>40.563136</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       Model  \\\n",
              "1      RandomForestRegressor   \n",
              "4                        SVR   \n",
              "7          LightGBMRegressor   \n",
              "2               XGBRegressor   \n",
              "0           LinearRegression   \n",
              "5      DecisionTreeRegressor   \n",
              "6  GradientBoostingRegressor   \n",
              "8               MLPRegressor   \n",
              "3        KNeighborsRegressor   \n",
              "\n",
              "                                          BestParams       MAE        R2  \\\n",
              "1                         {'reg__n_estimators': 200}  3.614991  0.800781   \n",
              "4            {'reg__C': 10, 'reg__kernel': 'linear'}  3.936745  0.751142   \n",
              "7  {'reg__n_estimators': 200, 'reg__num_leaves': 50}  4.257365  0.736723   \n",
              "2    {'reg__max_depth': 6, 'reg__n_estimators': 200}  4.391310  0.724847   \n",
              "0                                                 {}  3.762273  0.709092   \n",
              "5                           {'reg__max_depth': None}  4.520351  0.655922   \n",
              "6  {'reg__learning_rate': 0.1, 'reg__n_estimators...  5.193518  0.635306   \n",
              "8  {'reg__alpha': 0.001, 'reg__hidden_layer_sizes...  4.812238  0.632065   \n",
              "3                            {'reg__n_neighbors': 5}  5.815198  0.506143   \n",
              "\n",
              "   TrainTime_s  \n",
              "1  2576.957614  \n",
              "4  3885.292482  \n",
              "7  1637.035079  \n",
              "2     5.309225  \n",
              "0    33.442690  \n",
              "5    19.986360  \n",
              "6    26.459972  \n",
              "8  1402.680601  \n",
              "3    40.563136  "
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results_reg_df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mostramos una comparación detallada entre distintos modelos de regresión en términos de error absoluto medio (MAE), coeficiente de determinación (R²) y tiempo de entrenamiento (`TrainTime_s`). El modelo RandomForestRegressor se posiciona como el de mejor rendimiento, obteniendo el MAE más bajo (3.61) y el R² más alto (0.80), lo cual indica una buena capacidad para explicar la variabilidad de la variable `contribution_score` con una precisión razonable. Sin embargo, esto viene acompañado de un costo computacional considerable (2576 segundos).\n",
        "\n",
        "Modelos como SVR, LightGBMRegressor y XGBRegressor también obtienen resultados competitivos con R² por encima de 0.70, aunque difieren significativamente en el tiempo de entrenamiento: LightGBM muestra un buen balance entre desempeño (R² = 0.74) y eficiencia (1637 s), mientras que XGB se entrena mucho más rápido (5.3 s) aunque con un R² algo menor (0.72). Modelos como LinearRegression ofrecen rapidez (33 s) y una base razonable (R² = 0.70), pero su capacidad predictiva es superada por modelos no lineales. Finalmente, MLPRegressor, GradientBoostingRegressor y KNeighborsRegressor presentan desempeños más limitados, lo que sugiere que no capturan tan bien los patrones del problema en comparación con modelos basados en árboles más robustos.\n"
      ],
      "metadata": {
        "id": "H5EXbTIXAU6T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento de modelos para clasificación múltiple\n",
        "\n",
        "Se define un diccionario de configuración para los modelos multiclase. En este caso, se utilizan dos clasificadores: `RandomForestClassifier` y `XGBClassifier`, junto con sus respectivos espacios de búsqueda de hiperparámetros para ajuste posterior mediante GridSearchCV"
      ],
      "metadata": {
        "id": "hxOJa3-aBQKR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9721825-7e4f-4560-a8fb-41912c6a82f9",
        "msg_id": "5d38b7ee-3304-490a-a6f1-8eb248d1e6e1"
      },
      "outputs": [],
      "source": [
        "model_configs_multi = {\n",
        "    'RandomForest': (\n",
        "        RandomForestClassifier(random_state=42),\n",
        "        {'clf__n_estimators': [100, 200], 'clf__max_depth': [None, 10, 20]}\n",
        "    ),\n",
        "    'XGBoost': (\n",
        "        XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),\n",
        "        {'clf__n_estimators': [100, 200], 'clf__max_depth': [3, 6]}\n",
        "    )\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgLKhEin5K5o"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le_multi = LabelEncoder()\n",
        "ymulti_train_encoded = le_multi.fit_transform(ymulti_train)\n",
        "ymulti_test_encoded = le_multi.transform(ymulti_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se inicializan dos litas vacías para almacenar los resultados de desempeño y los mejores modelos obtenidos durante la búsqueda.\n",
        "\n",
        "Se crea un pipeline que incluye tanto el preprocesamiento como el modelo. Esto garntiza que todo el flujo de transformación se mantenga consistente durante la validación cruzada.\n",
        "\n",
        " Se entrena cada modelo utilizando búsqueda en rejilla (`GridSearchCV`) con validación cruzada de 3 folds. Se utiliza como métrica principal la `accuracy`. También se registra el tiempo de entrenamiento con `time.time()` para análisis posterior.\n",
        "\n",
        " Una vez entrenado, se recupera el mejor estimador según la búsqueda y se realizan predicciones obre el conjunto de prueba.\n",
        "\n",
        " Se evalúa el desempeño del modelo utilizando accuracy y F1 ponderado, lo cual es útil en clases desbalanceadas.\n",
        "\n"
      ],
      "metadata": {
        "id": "GZ8bEzykB5CD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d20174c0-6244-4dd3-a4f1-d668e8c5afdb",
        "msg_id": "a8a991e6-4bff-4362-be49-25549d2d7764",
        "outputId": "20709bbd-df7e-4704-ca31-0d3e0c684c77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entrenando clasificador multiclase: RandomForest\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RandomForest: Accuracy=0.724, F1=0.693, Time=138.1s\n",
            "Entrenando clasificador multiclase: XGBoost\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/alt9193/Documents/act4/.venv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [12:40:22] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/alt9193/Documents/act4/.venv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [12:40:22] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/alt9193/Documents/act4/.venv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [12:40:22] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/alt9193/Documents/act4/.venv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [12:40:22] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/alt9193/Documents/act4/.venv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [12:40:22] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/alt9193/Documents/act4/.venv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [12:40:22] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/alt9193/Documents/act4/.venv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [12:40:22] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/alt9193/Documents/act4/.venv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [12:40:22] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/alt9193/Documents/act4/.venv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [12:40:22] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/alt9193/Documents/act4/.venv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [12:40:22] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/alt9193/Documents/act4/.venv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [12:40:22] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/alt9193/Documents/act4/.venv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [12:40:22] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/alt9193/Documents/act4/.venv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [12:40:27] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XGBoost: Accuracy=0.744, F1=0.690, Time=7.0s\n",
            "          Model                                         BestParams  Accuracy  \\\n",
            "1       XGBoost    {'clf__max_depth': 3, 'clf__n_estimators': 100}  0.744077   \n",
            "0  RandomForest  {'clf__max_depth': None, 'clf__n_estimators': ...  0.724463   \n",
            "\n",
            "         F1  TrainTime_s  \n",
            "1  0.689755     7.043153  \n",
            "0  0.693244   138.119574  \n"
          ]
        }
      ],
      "source": [
        "results_multi = []\n",
        "best_models_multi = {}\n",
        "\n",
        "for name, (model, param_grid) in model_configs_multi.items():\n",
        "    print(f\"Entrenando clasificador multiclase: {name}\")\n",
        "    pipe = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('clf', model)\n",
        "    ])\n",
        "    grid = GridSearchCV(pipe, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
        "    start = time.time()\n",
        "    grid.fit(X_train, ymulti_train_encoded)\n",
        "    elapsed = time.time() - start\n",
        "    best = grid.best_estimator_\n",
        "    y_pred = best.predict(X_test)\n",
        "    acc = accuracy_score(ymulti_test_encoded, y_pred)\n",
        "    f1 = f1_score(ymulti_test_encoded, y_pred, average='weighted')\n",
        "    results_multi.append({\n",
        "        'Model': name,\n",
        "        'BestParams': grid.best_params_,\n",
        "        'Accuracy': acc,\n",
        "        'F1': f1,\n",
        "        'TrainTime_s': elapsed\n",
        "    })\n",
        "    best_models_multi[name] = best\n",
        "    print(f\"{name}: Accuracy={acc:.3f}, F1={f1:.3f}, Time={elapsed:.1f}s\")\n",
        "\n",
        "# Guardar resultados en DataFrame\n",
        "df_multi = pd.DataFrame(results_multi).sort_values('Accuracy', ascending=False)\n",
        "print(df_multi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1643624-bec7-4b1d-bbe6-9bbfd90813e5",
        "msg_id": "f5811095-2670-48a1-ade2-052f61fcde31",
        "outputId": "8cc1480c-fe72-4a12-a46e-7348843ace53"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>BestParams</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>TrainTime_s</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>{'clf__max_depth': 3, 'clf__n_estimators': 100}</td>\n",
              "      <td>0.744077</td>\n",
              "      <td>0.689755</td>\n",
              "      <td>7.043153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>RandomForest</td>\n",
              "      <td>{'clf__max_depth': None, 'clf__n_estimators': ...</td>\n",
              "      <td>0.724463</td>\n",
              "      <td>0.693244</td>\n",
              "      <td>138.119574</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Model                                         BestParams  Accuracy  \\\n",
              "1       XGBoost    {'clf__max_depth': 3, 'clf__n_estimators': 100}  0.744077   \n",
              "0  RandomForest  {'clf__max_depth': None, 'clf__n_estimators': ...  0.724463   \n",
              "\n",
              "         F1  TrainTime_s  \n",
              "1  0.689755     7.043153  \n",
              "0  0.693244   138.119574  "
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_multi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  Accuracy: XGBoost alcanza una mayor precisión general (74.4%) comparado con Random Forest (72.4%), lo que indica una mejor capacidad de clasificar correctamente las tres clases (Bajo, Medio, Alto).\n",
        "\n",
        "*  F1 Score ponderado: Ambos modelos obtienen valores similares (~0.69), lo que sugiere un desempeño comparable cuando se consideran tanto precisión como recall, ponderado por la frecuencia de cada clase.\n",
        "\n",
        "*  Eficiencia computacional: El modelo XGBoost es drásticamente más rápido, con un tiempo de entrenamiento de solo 7 segundos, frente a los 138 segundos del Random Forest. Esto puede ser crucial si se busca eficiencia para futuros reentrenamientos o despliegue.\n",
        "\n",
        "\n",
        "\n",
        "Aunque ambos modelos muestran un rendimiento similar en cuanto a métricas, XGBoost se posiciona como la mejor opción en este caso al ofrecer una mayor precisión con un tiempo de entrenamiento más de 90% menor, lo que lo convierte en una alternativa eficiente y competitiva para tareas de clasificación multiclase en este proyecto."
      ],
      "metadata": {
        "id": "Jb4Z7pmHD3Sg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clasificación binaria\n",
        "\n",
        "Se definen tres modelos binarios (lineal y dos árboles) junto con sus respectivos hiperparámetros para optimizar con `GridSearchCV`"
      ],
      "metadata": {
        "id": "boP-ums-E939"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4e60f898-03ab-4397-bfd4-20d26dc8dbcf",
        "msg_id": "2e2926b0-5b44-4b75-8d21-cc4d1e340de4"
      },
      "outputs": [],
      "source": [
        "model_configs_bin = {\n",
        "    'LogisticRegression': (\n",
        "        LogisticRegression(max_iter=1000, random_state=42),\n",
        "        {'clf__C': [0.1, 1, 10]}\n",
        "    ),\n",
        "    'RandomForest': (\n",
        "        RandomForestClassifier(random_state=42),\n",
        "        {'clf__n_estimators': [100, 200], 'clf__max_depth': [None, 10, 20]}\n",
        "    ),\n",
        "    'XGBoost': (\n",
        "        XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),\n",
        "        {'clf__n_estimators': [100, 200], 'clf__max_depth': [3, 6]}\n",
        "    )\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3phQ51vg5K5r"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import log_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para cada modelo, se construye un pipeline que incluye el preprocesamiento (`ColumnTransformer`) y el modelo (`clf`). Se realiza búsqueda en rejilla (`GridSearchCV`) para optimizar los hiperparámetros en validación cruzada (cv=3). También se mide el tiempo de entrenamiento.\n",
        "\n",
        "Una vez entrenado el mejor modelo, se generan predicciones sobre el conjunto de prueba (`X_test`). Se calculan métricas clave para clasificación binaria."
      ],
      "metadata": {
        "id": "UvKD1PJIFOgB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9f3c20fe-0c25-4412-971e-c03d21bb7300",
        "msg_id": "3d143df2-d242-41c0-89eb-4f51bd8ba79d",
        "outputId": "703f2070-c06c-482d-c450-55d203fa01d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entrenando clasificador binario: LogisticRegression\n",
            "LogisticRegression: Accuracy=0.740, F1=0.740, AUC=0.844, LogLoss=0.478, Time=6.2s\n",
            "Entrenando clasificador binario: RandomForest\n",
            "RandomForest: Accuracy=0.734, F1=0.745, AUC=0.847, LogLoss=0.459, Time=80.3s\n",
            "Entrenando clasificador binario: XGBoost\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/alt9193/Documents/act4/.venv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [12:43:56] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/alt9193/Documents/act4/.venv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [12:43:56] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/alt9193/Documents/act4/.venv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [12:43:56] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/alt9193/Documents/act4/.venv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [12:43:56] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/alt9193/Documents/act4/.venv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [12:43:56] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/alt9193/Documents/act4/.venv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [12:43:56] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/alt9193/Documents/act4/.venv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [12:43:56] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/alt9193/Documents/act4/.venv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [12:43:56] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/alt9193/Documents/act4/.venv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [12:43:56] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/alt9193/Documents/act4/.venv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [12:43:57] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/alt9193/Documents/act4/.venv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [12:43:57] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/alt9193/Documents/act4/.venv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [12:43:57] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/alt9193/Documents/act4/.venv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [12:43:58] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XGBoost: Accuracy=0.691, F1=0.747, AUC=0.825, LogLoss=0.495, Time=3.6s\n",
            "                Model                                         BestParams  \\\n",
            "0  LogisticRegression                                     {'clf__C': 10}   \n",
            "1        RandomForest  {'clf__max_depth': None, 'clf__n_estimators': ...   \n",
            "2             XGBoost    {'clf__max_depth': 6, 'clf__n_estimators': 200}   \n",
            "\n",
            "   Accuracy        F1       AUC   LogLoss  TrainTime_s  \n",
            "0  0.739840  0.740044  0.843740  0.478348     6.153708  \n",
            "1  0.734348  0.745069  0.847000  0.458986    80.254101  \n",
            "2  0.690570  0.746725  0.824876  0.494897     3.566475  \n"
          ]
        }
      ],
      "source": [
        "results_bin = []\n",
        "best_models_bin = {}\n",
        "\n",
        "for name, (model, param_grid) in model_configs_bin.items():\n",
        "    print(f\"Entrenando clasificador binario: {name}\")\n",
        "    pipe = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('clf', model)\n",
        "    ])\n",
        "    grid = GridSearchCV(pipe, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
        "    start = time.time()\n",
        "    grid.fit(X_train, ybin_train)\n",
        "    elapsed = time.time() - start\n",
        "    best = grid.best_estimator_\n",
        "    y_pred = best.predict(X_test)\n",
        "    y_proba = best.predict_proba(X_test)[:, 1]\n",
        "    acc = accuracy_score(ybin_test, y_pred)\n",
        "    f1 = f1_score(ybin_test, y_pred)\n",
        "    auc = roc_auc_score(ybin_test, y_proba)\n",
        "    loss = log_loss(ybin_test, y_proba)\n",
        "    results_bin.append({\n",
        "        'Model': name,\n",
        "        'BestParams': grid.best_params_,\n",
        "        'Accuracy': acc,\n",
        "        'F1': f1,\n",
        "        'AUC': auc,\n",
        "        'LogLoss': loss,\n",
        "        'TrainTime_s': elapsed\n",
        "    })\n",
        "    best_models_bin[name] = best\n",
        "    print(f\"{name}: Accuracy={acc:.3f}, F1={f1:.3f}, AUC={auc:.3f}, LogLoss={loss:.3f}, Time={elapsed:.1f}s\")\n",
        "\n",
        "# Guardar resultados en DataFrame\n",
        "df_bin = pd.DataFrame(results_bin).sort_values('Accuracy', ascending=False)\n",
        "print(df_bin)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bbdd55c-662b-42f4-9a73-862552453961",
        "msg_id": "da7bff8f-0180-4e9d-a8de-45872dd993c0",
        "outputId": "23b3967a-a9d8-4971-82dc-f59326a967d2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>BestParams</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>AUC</th>\n",
              "      <th>LogLoss</th>\n",
              "      <th>TrainTime_s</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LogisticRegression</td>\n",
              "      <td>{'clf__C': 10}</td>\n",
              "      <td>0.739840</td>\n",
              "      <td>0.740044</td>\n",
              "      <td>0.843740</td>\n",
              "      <td>0.478348</td>\n",
              "      <td>6.153708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RandomForest</td>\n",
              "      <td>{'clf__max_depth': None, 'clf__n_estimators': ...</td>\n",
              "      <td>0.734348</td>\n",
              "      <td>0.745069</td>\n",
              "      <td>0.847000</td>\n",
              "      <td>0.458986</td>\n",
              "      <td>80.254101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>{'clf__max_depth': 6, 'clf__n_estimators': 200}</td>\n",
              "      <td>0.690570</td>\n",
              "      <td>0.746725</td>\n",
              "      <td>0.824876</td>\n",
              "      <td>0.494897</td>\n",
              "      <td>3.566475</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                Model                                         BestParams  \\\n",
              "0  LogisticRegression                                     {'clf__C': 10}   \n",
              "1        RandomForest  {'clf__max_depth': None, 'clf__n_estimators': ...   \n",
              "2             XGBoost    {'clf__max_depth': 6, 'clf__n_estimators': 200}   \n",
              "\n",
              "   Accuracy        F1       AUC   LogLoss  TrainTime_s  \n",
              "0  0.739840  0.740044  0.843740  0.478348     6.153708  \n",
              "1  0.734348  0.745069  0.847000  0.458986    80.254101  \n",
              "2  0.690570  0.746725  0.824876  0.494897     3.566475  "
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_bin"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*    Logistic Regression logra el mayor valor de AUC (0.8437), lo cual es especialmente relevante en problemas con clases desbalanceadas, ya que mide la capacidad del modelo para separar las clases. También alcanza una buena accuracy (0.7398) y un F1 cercano (0.7400), lo que indica un desempeño balanceado entre precisión y recall. Además, su tiempo de entrenamiento es muy bajo (~6 segundos), lo cual lo hace eficiente.\n",
        "\n",
        "*    Random Forest obtiene la mayor F1-score (0.7451), lo que sugiere un mejor equilibrio general entre precisión y recall que LogisticRegression, aunque con un costo computacional mayor (80 segundos). Además, presenta el mejor LogLoss (0.4589), lo cual indica buena calibración de probabilidades.\n",
        "\n",
        "*    XGBoost tiene el mejor tiempo de entrenamiento (~3.5 segundos), pero su accuracy y AUC son los más bajos de los tres modelos. Aunque su F1 es comparable (0.7467), el LogLoss es el peor (0.4948), lo que indica menor calidad en la estimación de probabilidades.\n",
        "\n",
        "\n",
        "Logistic Regression ofrece un excelente balance entre desempeño, estabilidad y eficiencia computacional, siendo el modelo más razonable. No obstante, si se da prioridad a la calidad del modelo en tareas críticas de clasificación (y hay tolerancia al tiempo de entrenamiento), Random Forest podría considerarse como candidato final, especialmente tras un ajuste adicional."
      ],
      "metadata": {
        "id": "pAqy1a6lFuPq"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}